{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conver Format to yolo\n",
    "현재 어노테이션 정보들은 모두 절대좌표로 찍혀 있는 상태이다. 이를 정규화된 상대 좌표로 변환한다.\n",
    "\n",
    "### Yolo Annotation format\n",
    "yolo dataset의 경우 이미지 하나에 annotation 정보와 클래스 넘버를 포함하고 있는 txt파일 하나가 매핑되는게 일반적이며, txt파일은 아래와 같이 구성된다.\n",
    "\n",
    "```\n",
    "클래스 번호   x좌표(어노테이션 박스 중심점)   y좌표(어노테이션 박스 중심점)    annotation box's width    annotation box's height\n",
    "```\n",
    "\n",
    "### AI Hub's annotation file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "labels_dir = \"./new_dataset/labels\"  # yolo format으로 저장될 위치\n",
    "imgs_dir = \"./new_dataset/images\"\n",
    "cnt = {0:0, 5:0, 6:0}\n",
    "class_num = {0:0, 5:1, 6:2}  # 0 : 정상(0번) 5 : 검은 썩음병(1번) 6 : 배추노균병(2번)\n",
    "\n",
    "image_filenames = [i.replace(\"\\\\\", \"/\") for i in glob(r\"D:\\origin_dataset\\images\\*\\*\")]\n",
    "json_filenames = [i.replace(\"\\\\\", \"/\") for i in glob(r\"D:\\origin_dataset\\labels\\*\\*.json\")]\n",
    "for json_path, image_path in zip(json_filenames, image_filenames):\n",
    "    with open(json_path, \"r\", encoding=\"utf8\") as file:\n",
    "        json_file = json.load(file)\n",
    "\n",
    "    image_filename = json_file[\"description\"][\"image\"]\n",
    "    img_extension = image_filename[image_filename.rfind(\".\") : ]\n",
    "    image_width = json_file['description']['width']\n",
    "    image_height = json_file['description']['height']\n",
    "\n",
    "    cnt[json_file[\"annotations\"]['disease']] += 1\n",
    "\n",
    "    if cnt[json_file[\"annotations\"]['disease']] <= 592:\n",
    "        bboxes = \"\"  # 바운딩 박스 좌표를 담을 빈 문자열\n",
    "        for point in json_file[\"annotations\"][\"points\"]:  # 모든 바운딩 박스 좌표를 순회하면서\n",
    "            x_tl, y_tl = point[\"xtl\"], point[\"ytl\"]  # x,y Top-left 좌표\n",
    "            x_br, y_br = point[\"xbr\"], point[\"ybr\"]  # x,y Bottom-right 좌표\n",
    "\n",
    "            bounding_box_width = (x_br - x_tl) / image_width  # 바운딩 박스 가로 길이 정규화\n",
    "            bounding_box_height = (y_br - y_tl) / image_height  # 바운딩 박스 세로 길이 정규화\n",
    "            center_x = int((x_tl + x_br) / 2)  # 바운딩 박스 중심점의 x좌표\n",
    "            center_y = int((y_tl + y_br) / 2)  # 바운딩 박스 중심점의 y좌표\n",
    "\n",
    "            normed_center_x = center_x/image_width  # 정규화된 바운딩 박스 중심점의 x좌표\n",
    "            normed_center_y = center_y/image_height  # 정규화된 바운딩 박스 중심점의 y좌표\n",
    "            bbox = f\"{class_num[json_file['annotations']['disease']]} {normed_center_x} {normed_center_y} {bounding_box_width} {bounding_box_height}\\n\"\n",
    "            bboxes += bbox\n",
    "\n",
    "        txt_filename = os.path.join(labels_dir, image_filename)\n",
    "        with open(txt_filename.replace(img_extension, \".txt\"), \"w\", encoding=\"utf8\") as txt:\n",
    "            txt.write(bboxes)\n",
    "        shutil.move(image_path, os.path.join(imgs_dir, image_filename))\n",
    "\n",
    "    file.close()\n",
    "    txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 평균 가로 길이 : 3679px\n",
      "이미지 평균 세로 길이 : 3233px\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "\n",
    "json_filenames = [i.replace(\"\\\\\", \"/\") for i in glob(r\"D:\\origin_dataset\\labels\\*\\*.json\")]\n",
    "\n",
    "total_width = 0\n",
    "total_height = 0\n",
    "for json_path in json_filenames:\n",
    "    with open(json_path, \"r\", encoding=\"utf8\") as file:\n",
    "        json_file = json.load(file)\n",
    "    image_width = json_file['description']['width']\n",
    "    image_height = json_file['description']['height']\n",
    "\n",
    "    total_width += image_width\n",
    "    total_height += image_height\n",
    "\n",
    "    file.close()\n",
    "\n",
    "print(f\"이미지 평균 가로 길이 : {int(total_width / len(json_filenames))}px\")\n",
    "print(f\"이미지 평균 세로 길이 : {int(total_height / len(json_filenames))}px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "\n",
    "json_filenames = [i.replace(\"\\\\\", \"/\") for i in glob(r\"./detection_dataset/*/labels/*/*.json\")]\n",
    "\n",
    "cnt = {0:0, 5:0, 6:0}\n",
    "for json_path in json_filenames:\n",
    "    with open(json_path, \"r\", encoding=\"utf8\") as file:\n",
    "        json_file = json.load(file)\n",
    "    if len(json_file[\"annotations\"][\"points\"]) >= 1:\n",
    "        cnt[json_file[\"annotations\"][\"disease\"]] += 1\n",
    "    else:\n",
    "        print(f\"thers's no any points. filename is {json_file['description']['image']}\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1183, 5: 1760, 6: 592}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 3552 files [02:06, 28.00 files/s] \n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "\n",
    "splitfolders.ratio(\"./new_dataset\", \"./new_detection\", seed=11, ratio=(0.6, 0.2, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V8 training params\n",
    "\n",
    "|params|default|설명|\n",
    "|---|---|---|\n",
    "|model|None|사전에 학습된 모델의 가중치를 사용하거나, 모델 자체를 사용하기 위한 parameter|\n",
    "|data|None|데이터셋 명세가 적힌 yaml 파일의 경로|\n",
    "|epochs|100|훈련 횟수|\n",
    "|time|None|학습 시간에 대한 limit을 설정한다.|\n",
    "|patience|100|early stop을 위한 매개변수. 해당 횟수만큼 모델에 개선이 없으면 조기종료한다.|\n",
    "|imgsz|640|이미지의 가로 세로 크기를 조절한다. 학습 정확도와 계산 복잡성에 영향을 직접적으로 미친다.|\n",
    "|save|True|학습 결과를 best/last.pt로 저장한다.|\n",
    "|save_preiod|-1|학습 중 checkpoint를 저장할지 말지에 대한 여부. -1은 비활성화를 의미하며, 지정하는 숫자의 epochs마다 저장한다.|\n",
    "|cache|None|데이터셋 이미지를 메모리 혹은 디스크에 캐싱하게 하여 메모리 사용량은 증가하지만 교육 속도는 향상된다.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.2 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.196  Python-3.9.12 torch-2.1.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=./detection_model/yolov8l.pt, data=./new_detection/cabbage_data.yaml, epochs=25, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=2, project=detection_model, name=cabbage_detection, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.0, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=detection_model\\cabbage_detection7\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5585113  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "Model summary: 365 layers, 43632153 parameters, 43632137 gradients, 165.4 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "WARNING  ClearML installed but not initialized correctly, not logging this run. It seems ClearML is not configured on this machine!\n",
      "To get started with ClearML, setup your own 'clearml-server' or create a free account at https://app.clear.ml\n",
      "Setup instructions can be found here: https://clear.ml/docs\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir detection_model\\cabbage_detection7', view at http://localhost:6006/\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "WARNING  NMS time limit 0.550s exceeded\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "train: Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels...:   0%|          | 0/1065 [00:00<?, ?it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 26 images, 0 backgrounds, 0 corrupt:   2%|▏         | 26/1065 [00:00<00:04, 248.79it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 106 images, 0 backgrounds, 0 corrupt:  10%|▉         | 106/1065 [00:00<00:01, 547.88it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 205 images, 0 backgrounds, 0 corrupt:  19%|█▉        | 205/1065 [00:00<00:01, 702.09it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 293 images, 0 backgrounds, 0 corrupt:  28%|██▊       | 293/1065 [00:00<00:01, 752.89it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 393 images, 0 backgrounds, 0 corrupt:  37%|███▋      | 393/1065 [00:00<00:00, 810.78it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 494 images, 0 backgrounds, 0 corrupt:  46%|████▋     | 494/1065 [00:00<00:00, 850.44it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 587 images, 0 backgrounds, 0 corrupt:  55%|█████▌    | 587/1065 [00:00<00:00, 849.91it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 672 images, 0 backgrounds, 0 corrupt:  63%|██████▎   | 672/1065 [00:00<00:00, 757.62it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 792 images, 0 backgrounds, 0 corrupt:  74%|███████▍  | 792/1065 [00:01<00:00, 875.56it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 882 images, 0 backgrounds, 0 corrupt:  83%|████████▎ | 882/1065 [00:01<00:00, 879.16it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 985 images, 0 backgrounds, 0 corrupt:  92%|█████████▏| 985/1065 [00:01<00:00, 896.28it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\train\\labels... 1065 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1065/1065 [00:01<00:00, 810.47it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\CudaTest\\cabbage\\new_detection\\train\\labels.cache\n",
      "val: Scanning D:\\CudaTest\\cabbage\\new_detection\\val\\labels...:   0%|          | 0/355 [00:00<?, ?it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\val\\labels... 72 images, 0 backgrounds, 0 corrupt:  20%|██        | 72/355 [00:00<00:00, 671.41it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\val\\labels... 152 images, 0 backgrounds, 0 corrupt:  43%|████▎     | 152/355 [00:00<00:00, 703.82it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\val\\labels... 246 images, 0 backgrounds, 0 corrupt:  69%|██████▉   | 246/355 [00:00<00:00, 774.39it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\val\\labels... 324 images, 0 backgrounds, 0 corrupt:  91%|█████████▏| 324/355 [00:00<00:00, 711.80it/s]Scanning D:\\CudaTest\\cabbage\\new_detection\\val\\labels... 355 images, 0 backgrounds, 0 corrupt: 100%|██████████| 355/355 [00:00<00:00, 714.25it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\CudaTest\\cabbage\\new_detection\\val\\labels.cache\n",
      "Plotting labels to detection_model\\cabbage_detection7\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mdetection_model\\cabbage_detection7\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/25      10.7G     0.9858      2.052      1.478         25        640: 100%|██████████| 67/67 [01:28<00:00,  1.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:10<00:00,  1.10it/s]\n",
      "                   all        355        355     0.0228      0.534     0.0161    0.00809\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/25      11.3G      1.171      1.609      1.583         22        640: 100%|██████████| 67/67 [01:18<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:12<00:00,  1.03s/it]\n",
      "                   all        355        355     0.0444      0.171     0.0175    0.00672\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/25      11.2G      1.208      1.545       1.59         26        640: 100%|██████████| 67/67 [01:18<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:25<00:00,  2.13s/it]\n",
      "                   all        355        355      0.149      0.156     0.0952     0.0354\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/25      11.3G      1.151      1.448      1.559         21        640: 100%|██████████| 67/67 [01:17<00:00,  1.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:11<00:00,  1.08it/s]\n",
      "                   all        355        355      0.285      0.365       0.24      0.113\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/25      11.3G      1.102       1.32      1.519         29        640: 100%|██████████| 67/67 [01:18<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:11<00:00,  1.08it/s]\n",
      "                   all        355        355      0.611      0.557      0.572      0.341\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/25      11.3G      1.037      1.266      1.471         21        640: 100%|██████████| 67/67 [01:17<00:00,  1.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:18<00:00,  1.52s/it]\n",
      "                   all        355        355      0.298      0.301      0.307      0.128\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/25      11.2G      1.022      1.188      1.453         26        640: 100%|██████████| 67/67 [01:18<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:10<00:00,  1.13it/s]\n",
      "                   all        355        355      0.752      0.739      0.778      0.495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/25      11.3G     0.9372      1.092       1.38         22        640: 100%|██████████| 67/67 [01:18<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:11<00:00,  1.05it/s]\n",
      "                   all        355        355      0.747      0.777      0.778      0.551\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/25      11.3G     0.9283      1.052      1.384         31        640: 100%|██████████| 67/67 [01:15<00:00,  1.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:26<00:00,  2.17s/it]\n",
      "                   all        355        355      0.825      0.754      0.841       0.57\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/25      11.3G      0.897      0.995      1.356         23        640: 100%|██████████| 67/67 [01:15<00:00,  1.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:07<00:00,  1.59it/s]\n",
      "                   all        355        355      0.764      0.826       0.87      0.638\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/25      11.3G     0.8668     0.9377      1.338         30        640: 100%|██████████| 67/67 [01:16<00:00,  1.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:09<00:00,  1.30it/s]\n",
      "                   all        355        355       0.81      0.834      0.891       0.66\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/25      11.3G     0.8602     0.9049      1.331         26        640: 100%|██████████| 67/67 [01:16<00:00,  1.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:16<00:00,  1.37s/it]\n",
      "                   all        355        355      0.821      0.796      0.875       0.63\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/25      11.3G     0.8236     0.8813      1.301         20        640: 100%|██████████| 67/67 [01:18<00:00,  1.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:12<00:00,  1.01s/it]\n",
      "                   all        355        355      0.772      0.819      0.862      0.646\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/25      11.3G     0.8083     0.8516      1.291         20        640: 100%|██████████| 67/67 [01:18<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:11<00:00,  1.08it/s]\n",
      "                   all        355        355      0.886      0.842      0.913      0.697\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/25      11.3G     0.7855     0.8174      1.269         21        640: 100%|██████████| 67/67 [01:18<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:23<00:00,  1.94s/it]\n",
      "                   all        355        355      0.824      0.851      0.897      0.696\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/25      11.3G     0.7182     0.7986      1.328          9        640: 100%|██████████| 67/67 [01:19<00:00,  1.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:16<00:00,  1.41s/it]\n",
      "                   all        355        355      0.867      0.849      0.898      0.695\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/25      11.3G     0.7086     0.6668      1.316          9        640: 100%|██████████| 67/67 [01:16<00:00,  1.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:11<00:00,  1.06it/s]\n",
      "                   all        355        355      0.837      0.854      0.904      0.665\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/25      11.3G     0.6324     0.5769      1.272          9        640: 100%|██████████| 67/67 [01:17<00:00,  1.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:24<00:00,  2.02s/it]\n",
      "                   all        355        355      0.904      0.887      0.937      0.764\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/25      11.3G     0.6379     0.5397      1.264          9        640: 100%|██████████| 67/67 [01:16<00:00,  1.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:10<00:00,  1.12it/s]\n",
      "                   all        355        355       0.86      0.876       0.93      0.749\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/25      11.3G     0.5877     0.5275      1.196          9        640: 100%|██████████| 67/67 [01:18<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:11<00:00,  1.08it/s]\n",
      "                   all        355        355      0.893      0.802      0.918      0.742\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/25      11.3G     0.5682     0.4881      1.189          9        640: 100%|██████████| 67/67 [01:16<00:00,  1.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:29<00:00,  2.48s/it]\n",
      "                   all        355        355      0.902      0.906      0.955      0.785\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/25      11.3G     0.5286      0.451      1.152          9        640: 100%|██████████| 67/67 [01:19<00:00,  1.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:08<00:00,  1.40it/s]\n",
      "                   all        355        355      0.915       0.87       0.95      0.799\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/25      11.3G     0.5034     0.4367      1.132          9        640: 100%|██████████| 67/67 [01:15<00:00,  1.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:09<00:00,  1.31it/s]\n",
      "                   all        355        355      0.868      0.917       0.95      0.811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/25      11.3G     0.4798      0.405      1.111          9        640: 100%|██████████| 67/67 [01:17<00:00,  1.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:16<00:00,  1.39s/it]\n",
      "                   all        355        355      0.894      0.923      0.961      0.825\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/25      11.3G     0.4576     0.3812      1.079          9        640: 100%|██████████| 67/67 [01:18<00:00,  1.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:12<00:00,  1.05s/it]\n",
      "                   all        355        355      0.906       0.93       0.96      0.823\n",
      "\n",
      "25 epochs completed in 0.667 hours.\n",
      "Optimizer stripped from detection_model\\cabbage_detection7\\weights\\last.pt, 87.6MB\n",
      "Optimizer stripped from detection_model\\cabbage_detection7\\weights\\best.pt, 87.6MB\n",
      "\n",
      "Validating detection_model\\cabbage_detection7\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.196  Python-3.9.12 torch-2.1.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "Model summary (fused): 268 layers, 43608921 parameters, 0 gradients, 164.8 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:08<00:00,  1.49it/s]\n",
      "                   all        355        355      0.894      0.923      0.961      0.825\n",
      "                normal        355        115      0.903      0.843      0.917      0.776\n",
      "             black_rot        355        130      0.859      0.935      0.977      0.795\n",
      "              syngenta        355        110       0.92      0.991      0.989      0.905\n",
      "Speed: 0.4ms preprocess, 15.3ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mdetection_model\\cabbage_detection7\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69906fd5762a4b5792a39ab8f472c00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▃▆███▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>lr/pg1</td><td>▃▆███▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>lr/pg2</td><td>▃▆███▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▁▁▂▃▅▃▇▇▇▇▇▇▇████████████</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁▁▁▂▄▂▅▆▆▆▇▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>metrics/precision(B)</td><td>▁▁▂▃▆▃▇▇▇▇▇▇▇█▇█▇████████</td></tr><tr><td>metrics/recall(B)</td><td>▄▁▁▃▅▂▆▇▆▇▇▇▇▇▇▇▇██▇█████</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>▆██▇▇▆▆▅▅▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁</td></tr><tr><td>train/cls_loss</td><td>█▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/dfl_loss</td><td>▆███▇▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▂▂▁▁</td></tr><tr><td>val/box_loss</td><td>▅██▆▄▅▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/cls_loss</td><td> █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/dfl_loss</td><td>█▇▆▄▃▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00013</td></tr><tr><td>lr/pg1</td><td>0.00013</td></tr><tr><td>lr/pg2</td><td>0.00013</td></tr><tr><td>metrics/mAP50(B)</td><td>0.96114</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.82533</td></tr><tr><td>metrics/precision(B)</td><td>0.89401</td></tr><tr><td>metrics/recall(B)</td><td>0.92306</td></tr><tr><td>model/GFLOPs</td><td>165.412</td></tr><tr><td>model/parameters</td><td>43632153</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>24.104</td></tr><tr><td>train/box_loss</td><td>0.45765</td></tr><tr><td>train/cls_loss</td><td>0.38122</td></tr><tr><td>train/dfl_loss</td><td>1.07859</td></tr><tr><td>val/box_loss</td><td>0.60857</td></tr><tr><td>val/cls_loss</td><td>0.48977</td></tr><tr><td>val/dfl_loss</td><td>1.15</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync d:\\CudaTest\\cabbage\\wandb\\offline-run-20240424_160500-on6t66zw<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\offline-run-20240424_160500-on6t66zw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "def main():\n",
    "    model = YOLO(\"./detection_model/yolov8l.pt\", task=\"detect\")\n",
    "    model.train(\n",
    "        model = \"./detection_model/yolov8l.pt\",\n",
    "        data = \"./new_detection/cabbage_data.yaml\",\n",
    "        imgsz=640,\n",
    "        epochs=25,\n",
    "        batch=16,  # batch=-1 -> 장비가 허용하는 범위 내에서 동적으로 설정\n",
    "        project=\"detection_model\",\n",
    "        name=\"cabbage_detection\",\n",
    "        workers=2,\n",
    "        plots=True,\n",
    "        device=0,  # gpu 사용\n",
    "        patience=10,\n",
    "        translate=0.0,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_0_00_03_03_12_0_b06_20201020_0005_S01_1.jpg: 320x640 1 normal, 28.4ms\n",
      "image 2/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_0_00_03_03_12_0_b06_20201020_0070_S01_1.jpg: 320x640 1 normal, 15.7ms\n",
      "image 3/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_0_00_03_03_12_0_b06_20201020_0072_S01_1.jpg: 320x640 1 normal, 31.9ms\n",
      "image 4/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_0_00_03_03_12_0_b06_20201020_0092_S01_1.jpg: 320x640 1 normal, 30.6ms\n",
      "image 5/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_1_05_03_03_12_1_5978e_20201029_12_a0002.jpeg: 640x640 1 black_rot, 51.7ms\n",
      "image 6/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_1_05_03_03_12_1_5978e_20201029_12_a0005.jpeg: 640x640 1 black_rot, 62.0ms\n",
      "image 7/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_1_05_03_03_12_1_5978e_20201029_12_a0007.jpeg: 640x640 1 black_rot, 81.5ms\n",
      "image 8/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_1_05_03_03_12_1_5978e_20201029_12_b0004.jpeg: 640x640 1 black_rot, 73.1ms\n",
      "image 9/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_1_06_03_03_12_2_5978e_20201005_26_a0007.jpg: 640x480 1 syngenta, 70.2ms\n",
      "image 10/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_1_06_03_03_12_2_5978e_20201127_127_a0004.jpg: 640x480 1 syngenta, 143.7ms\n",
      "image 11/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_1_06_03_03_12_2_7204t_20201103_46_a0004.jpg: 640x480 2 syngentas, 141.0ms\n",
      "image 12/12 C:\\Users\\admin\\Desktop\\testing\\V006_79_1_06_03_03_12_3_7204t_20201116_73_a0004.JPG: 480x640 1 syngenta, 39.3ms\n",
      "Speed: 2.6ms preprocess, 64.1ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mc:\\Users\\admin\\Desktop\\CudaTest\\runs\\detect\\predict5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "custom_model = YOLO(r\"C:\\Users\\admin\\Desktop\\CudaTest\\runs\\detect\\cabbage3\\weights\\best.pt\")\n",
    "results = custom_model.predict(source=r\"C:\\Users\\admin\\Desktop\\testing\\*\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for result in results:\n",
    "    uniq, cnt = np.unique(result.boxes.cls.cpu().numpy(), return_counts=True)\n",
    "    uniq_cnt_dict = dict(zip(uniq, cnt))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
